{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379c09d0-9de5-4ba1-bdd9-01879f638ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import groupby\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as nnf\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import clip\n",
    "\n",
    "import numpy as np\n",
    "from os.path import join, isdir, expanduser\n",
    "from PIL import Image\n",
    "\n",
    "data_path = expanduser('~/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf1aa206-1759-4cb4-abbb-0549e2c0b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A loading: 100%|██████████| 395/395 [00:01<00:00, 198.23it/s]\n",
      "B loading: 100%|██████████| 422/422 [00:02<00:00, 202.71it/s]\n",
      "A to Tensor: 100%|██████████| 393/393 [00:18<00:00, 21.48it/s]\n",
      "B to Tensor: 100%|██████████| 421/421 [00:23<00:00, 18.12it/s]\n",
      "A collecting metadata: 100%|██████████| 393/393 [00:14<00:00, 26.55it/s]\n",
      "B collecting metadata: 100%|██████████| 421/421 [00:15<00:00, 26.80it/s]\n",
      "A normalizing: 100%|██████████| 393/393 [00:21<00:00, 17.96it/s]\n",
      "B normalizing: 100%|██████████| 421/421 [00:19<00:00, 21.26it/s]\n"
     ]
    }
   ],
   "source": [
    "num_images = 2000\n",
    "test_size = 32\n",
    "size = (256,256)\n",
    "\n",
    "\n",
    "A_path = [\"share\", \"Florian_Jonas_construction\"]\n",
    "B_path = [\"share\", \"Florian_Jonas_finished\"]\n",
    "\n",
    "A_files = os.listdir(join(data_path, *A_path))\n",
    "B_files = os.listdir(join(data_path, *B_path))\n",
    "\n",
    "\n",
    "A_raw = [Image.open(join(data_path, *A_path, f)).resize(size).convert(\"RGB\")\n",
    "          for f in tqdm(A_files[:num_images], desc=\"A loading\") if f.endswith('.jpg')]\n",
    "\n",
    "B_raw = [Image.open(join(data_path, *B_path, f)).resize(size).convert(\"RGB\")\n",
    "          for f in tqdm(B_files[:num_images], desc=\"B loading\") if f.endswith('.jpg')]\n",
    "\n",
    "\n",
    "\n",
    "to_ten = transforms.ToTensor()\n",
    "# norm = transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n",
    "norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "\n",
    "A = [to_ten(img) for img in tqdm(A_raw, desc=\"A to Tensor\")]\n",
    "B = [to_ten(img) for img in tqdm(B_raw, desc=\"B to Tensor\")]\n",
    "\n",
    "A_meta = [[img.mean(dim=(1,2)), img.std(dim=(1,2))] for img in tqdm(A, desc=\"A collecting metadata\")]\n",
    "B_meta = [[img.mean(dim=(1,2)), img.std(dim=(1,2))] for img in tqdm(B, desc=\"B collecting metadata\")]\n",
    "\n",
    "A = [norm(img) for img in tqdm(A, desc=\"A normalizing\")]\n",
    "B = [norm(img) for img in tqdm(B, desc=\"B normalizing\")]\n",
    "\n",
    "torch.save(A[test_size:], \"A_old.pt\")\n",
    "torch.save(B[test_size:], \"B_old.pt\")\n",
    "\n",
    "# torch.save(A_meta[test_size:], \"A_meta.pt\")\n",
    "# torch.save(B_meta[test_size:], \"B_meta.pt\")\n",
    "\n",
    "\n",
    "\n",
    "torch.save(A[:test_size], \"A_old_test.pt\")\n",
    "torch.save(B[:test_size], \"B_old_test.pt\")\n",
    "\n",
    "# torch.save(A_meta[:test_size], \"A_meta_test.pt\")\n",
    "# torch.save(B_meta[:test_size], \"B_meta_test.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b834d85-bd14-41b8-ae0f-a3178ea6b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1335, 0.4496, 0.7550])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0].mean(dim=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "584fd425-cce1-4f80-9459-447fd37c705f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RGB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jonas_florian",
   "language": "python",
   "name": "jonas_florian"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
