{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5632a38-5d60-40ac-9942-1e3808cdae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from os.path import join, isdir, expanduser\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = expanduser('~/datasets')\n",
    "\n",
    "# need to adapt locally\n",
    "\n",
    "raw_image_path = [\"share\", \"raw_images\"]\n",
    "construction_path = [\"share\", \"construction\"]\n",
    "finished_path = [\"share\", \"finished\"]\n",
    "\n",
    "image_files = os.listdir(join(data_path, *raw_image_path))\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1298c-758a-4f9d-868c-4cfc04e7d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CLIP performance for keyword sentences with already correctly separated images \n",
    "\n",
    "num_images = 1000\n",
    "size = (256,256)\n",
    "\n",
    "# Prepare the inputs\n",
    "raw_image_batch = [Image.open(join(data_path, *raw_image_path, f))\n",
    "          for f in tqdm(image_files[:num_images], desc=\"Raw Images loading\") if f.endswith('.jpg')]\n",
    "\n",
    "# image = raw_image_batch[1]\n",
    "images = raw_image_batch\n",
    "\n",
    "classes=[\"building in construction\", \"finished building\"]\n",
    "\n",
    "pp_images = [preprocess(img) for img in images]\n",
    "\n",
    "for img in pp_images:\n",
    "    image_input = img.unsqueeze(0).to(device)\n",
    "    # image_input = preprocess(images).unsqueeze(0).to(device)\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a image of a {c}\") \n",
    "                                 for c in classes]).to(device)\n",
    "\n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Pick the top 5 most similar labels for the image\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(2)\n",
    "\n",
    "    # Print the result\n",
    "    print(\"\\nTop predictions:\\n\")\n",
    "    for value, index in zip(values, indices):\n",
    "        print(f\"{classes[index]}: {100 * value.item():.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ca355-a7e3-404c-925c-97fe1791100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26390/1803886471.py:22: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(join(data_path, \"share\", \"skyscraper.tsv\"),\n",
      "/tmp/ipykernel_26390/1803886471.py:22: DtypeWarning: Columns (12,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(join(data_path, \"share\", \"skyscraper.tsv\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "finished skyscraper: 78.81%\n",
      "skyscraper in construction: 20.89%\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "finished skyscraper: 84.33%\n",
      "skyscraper in construction: 15.36%\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "finished skyscraper: 49.44%\n",
      "skyscraper in construction: 49.44%\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "finished skyscraper: 84.77%\n",
      "skyscraper in construction: 15.20%\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "finished skyscraper: 73.97%\n",
      "skyscraper in construction: 25.17%\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "finished skyscraper: 58.45%\n",
      "skyscraper in construction: 41.43%\n",
      "\n",
      "Top predictions:\n",
      "\n",
      "finished skyscraper: 70.12%\n",
      "skyscraper in construction: 29.69%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "def save_file(path, filename):\n",
    "    if os.path.exists(join(path, filename)):\n",
    "        save_file(path, \"0\"+filename)\n",
    "    else:\n",
    "        file = open(join(path, filename), \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "\n",
    "\n",
    "classes=[\"skyscraper in construction\", \"finished skyscraper\", \"construction site\"]\n",
    "\n",
    "# data = pd.read_csv(join(data_path, \"share\", \"buildingconstruction.tsv\"),\n",
    "#                    sep=\"\\t\", error_bad_lines=False)\n",
    "# data = data.rename(columns={\n",
    "#                    'http://farm4.staticflickr.com/3055/2330466409_fc8133ec39.jpg': 'image_url'})\n",
    "\n",
    "data = pd.read_csv(join(data_path, \"share\", \"skyscraper.tsv\"),\n",
    "                   sep=\"\\t\", error_bad_lines=False)\n",
    "data = data.rename(columns={\n",
    "                   'http://farm3.staticflickr.com/2384/3543591719_b5f2cf8c98.jpg': 'image_url'})\n",
    "\n",
    "# data = data.rename(columns={\n",
    "#    'http://farm3.staticflickr.com/2384/3543591719_b5f2cf8c98.jpg': 'image_url'})\n",
    "data = data['image_url']\n",
    "# print(len(data.tolist()))\n",
    "\n",
    "for i, url in enumerate(data.tolist()[0:10000]):\n",
    "    # response = requests.get(\"https://i.imgur.com/ExdKOOz.png\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except requests.exceptions.MissingSchema as err:\n",
    "        print(err)\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    except UnidentifiedImageError:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    image_input = preprocess(img).unsqueeze(0).to(device)\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"a image of a {c}\") \n",
    "                                 for c in classes]).to(device)\n",
    "\n",
    "    # Calculate features\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    # Pick the top 5 most similar labels for the image\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity[0].topk(2)\n",
    "\n",
    "    # save image if it is more likely to be finished\n",
    "    if indices[0] == 1:\n",
    "        # save_file(join(data_path, *construction_path), f\"{i}.jpg\")\n",
    "        save_file(join(data_path, *finished_path), f\"{i}.jpg\")\n",
    "        \n",
    "\n",
    "        # Print the result\n",
    "        print(\"\\nTop predictions:\\n\")\n",
    "        for value, index in zip(values, indices):\n",
    "            print(f\"{classes[index]}: {100 * value.item():.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2941f137-e7f8-4794-a27f-3afde074781b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "895efb41-3420-4ad3-93da-73440d76b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "url = 'https://farm4.staticflickr.com/3055/2330466409_fc8133ec39.jpg'\n",
    "response = requests.get(url)\n",
    "file = open(join(data_path, *construction_path, f\"{1}.jpg\"), \"wb\")\n",
    "file.write(response.content)\n",
    "file.close()\n",
    "\n",
    "# img = Image.open(BytesIO(response.content))\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66903aac-5dc5-4ff9-a922-e22b6ec58a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.jpg\n"
     ]
    }
   ],
   "source": [
    "image_files = os.listdir(join(data_path, *construction_path))\n",
    "for i in image_files:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd372705-4e43-4d88-a029-e97d88cfa393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
